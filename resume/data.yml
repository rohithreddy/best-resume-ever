/* #*/ export const PERSON = `
name:
  first: Rohith
  middle:
  last: Reddy
about: A Programmer with 4+ years of experience in building Data Intensive Applications that are simple and functional, tackling challenging architectural and scalability problems. In my free time I am either hacking Linux kernel or building embedded IoT devices.
# position: Software Programmer

computer_skills:
- type: Languages
  list: Python, Javascript, Java
- type: Frameworks
  list: Flask, Falcon, Pandas, Backbone.js, Airflow, Spring, Spark, Kafka, Chalice, Selenium, Dask
- type: Databases
  list: MySQL, PostgreSQL, MongoDB, Druid, DynamoDB, Redshift
- type: Dev tools
  list: Git, Nginx, Shell, Intellij, VS Code, Docker, AWS CDK
- type: Hosting
  list: AWS, GCP



experience:
- company: Sphota
  place: Bangalore
  position: Member of Technical Staff
  timeperiod: Sep 2018 - Aug 2019
  tech_stack: Falcon, PostgresQL, Python, PubSub, CloudFunctions, CloudScheduler, Vegeta
  product_name: Paid Search Platform
  url: https://sphota.biz
  description: 
    - d_item: Developed high-performance ad trackers using Falcon, PubSub for multi-touch attribution modeling
    - d_item: Designed and developed scalable processing pipeline based on Google Cloud Functions and Cloud scheduler
    - d_item: Built reporting system that helps marketers to gain insights on Ad Spends
    - d_item: Made changes related to GDPR in internal data platform for Fortune 500 client

- company: Niyo
  place: Bangalore
  position: Data Engineer
  timeperiod: Jun 2017 - Apr 2018
  tech_stack: Airflow, S3, Redshift, Metabase, Python, Pandas, Kafka, React.js, Spring, EC2
  product_name: Niyo Fintech Products
  url: https://www.goniyo.com
  description: 
    - d_item: Built centralized data warehouse that handles transactional data generated by 3lakh customers users based on Redshift
    - d_item: Developed business intelligence system on Metabase, trained business and tech users in using business intelligence system, curated access and analyses helping our organization be data-driven
    - d_item: created investor dashboards that helped raise funding of $14 million
    - d_item: Implemented data quality checks found massive bugs in partner's services that were causing erroneous transactions
    - d_item: Automated sharing data with partners and various reports for invoicing using Airflow


- company: RedBlackTree
  place: Chennai
  position: Serverside IOT Engineer
  timeperiod: Oct 2016 - Feb 2017
  tech_stack: Python, Druid, Pandas, Flask, S3, Zookeeper, Vue.js
  product_name: Predictive Maintenance Platform
  url: https://www.ascendo.ai
  description: 
    - d_item: Mapped data from various customer sources including CRM, service history, knowledge base to metadata model
    - d_item: Designed Druid based solution improving latency to under sub-second from usual timeouts
    - d_item: Created data pipeline to ingest data to Druid using S3, Pandas
    - d_item: Designed and implemented restful APIs for consuming data from front-end using Flask, Pandas
  
- company: Customer Centria
  place: Bangalore
  position: Software Engineer
  timeperiod: Jul 2015 - Aug 2016


education:
- degree: M.Sc Physics, BITS Pilani University
  location: Goa
  timeperiod: 2009 - 2015

recentprojects:
- tech_stack: Falcon, PostgresQL, Python, PubSub, CloudFunctions, CloudScheduler, Vegeta
  year: 2019
  product_name: Paid Search Platform
  url: https://sphota.biz
  description: 
    - d_item: Developed high-performance ad trackers using Falcon, PubSub for multi-touch attribution modeling
    - d_item: Designed and developed scalable processing pipeline based on Google Cloud Functions and Cloud scheduler
    - d_item: Built reporting system that helps marketers to gain insights on Ad Spends
    - d_item: Made changes related to GDPR in internal data platform for Fortune 500 client
- tech_stack: Airflow, S3, Redshift, Metabase, Python, Pandas, Kafka, React.js, Spring, EC2
  year: 2018
  product_name: Niyo Fintech Products
  url: https://www.goniyo.com
  description: 
    - d_item: Built centralized data warehouse that handles transactional data generated by 3lakh customers users based on Redshift
    - d_item: Developed business intelligence system on Metabase, trained business and tech users in using business intelligence system, curated access and analyses helping our organization be data-driven
    - d_item: created investor dashboards that helped raise funding of $14 million
    - d_item: Implemented data quality checks found massive bugs in partner's services that were causing erroneous transactions
    - d_item: Automated sharing data with partners and various reports for invoicing using Airflow
- tech_stack: Python, Druid, Pandas, Flask, S3, Zookeeper, Vue.js
  year: 2017
  product_name: Predictive Maintenance Platform
  url: https://www.ascendo.ai
  description: 
    - d_item: Mapped data from various customer sources including CRM, service history, knowledge base to metadata model
    - d_item: Designed Druid based solution improving latency to under sub-second from usual timeouts
    - d_item: Created data pipeline to ingest data to Druid using S3, Pandas
    - d_item: Designed and implemented restful APIs for consuming data from front-end using Flask, Pandas


contact:
  email: rohith.reddi9@gmail.com
  phone: +91 888 654 0155
  github: rohithreddy
  website: rohithreddy.github.io
# en, de, fr, pt, ca, cn, it, es, th, pt-br, ru, sv, id, hu, pl, ja, ka, nl, he, zh-tw, lt, ko, el, nb-no
lang: en
`